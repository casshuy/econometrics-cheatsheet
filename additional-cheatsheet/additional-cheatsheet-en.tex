\documentclass[10pt, a4paper, landscape]{extarticle}

% -----packages-----
\usepackage{multicol} % for multiple columns
\usepackage[landscape]{geometry} % for landscape
\usepackage{parskip} % remove text indentation
\usepackage{graphicx} % for scale tables
\usepackage{enumitem} % indent of lists
\usepackage{tikz} % for plots
\usepackage{hyperref} % for hyperlinks
\usepackage{amsmath} % for writing normal text on equations
\usepackage{scrlayer-scrpage} % page foot
\usepackage[compact]{titlesec} % titles spacing

% -----custom commands-----
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\se}{\mathrm{se}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Corr}{\mathrm{Corr}}
\newcommand{\SSR}{\mathrm{SSR}}
\newcommand{\SSE}{\mathrm{SSE}}
\newcommand{\SST}{\mathrm{SST}}
\newcommand{\Cr}{\mathrm{Cr}}
\newcommand{\AIC}{\mathrm{AIC}}
\newcommand{\HQ}{\mathrm{HQ}}
\newcommand{\Sc}{\mathrm{Sc}}

% -----page customization-----
\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} % margins configuration
\pagenumbering{gobble} % remove page numeration
\setlength{\parskip}{0cm} % paragraph skip length
% title spacing:
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}

% -----document-----
\begin{document}

% page foot
\cfoot{\href{https://github.com/marcelomijas/econometrics-cheatsheet}{\normalfont \footnotesize Version add0.3-en - github.com/marcelomijas/econometrics-cheatsheet - CC BY 4.0}}
\setlength{\footskip}{12pt}

\begin{multicols}{3} % set columns to 3

\begin{center}
	\textbf{\LARGE \href{https://github.com/marcelomijas/econometrics-cheatsheet}{Additional Cheat Sheet}} \\
	{\footnotesize By Marcelo Moreno - King Juan Carlos University} \\
	{\footnotesize As part of the Econometrics Cheat Sheet Project}
\end{center}

\section*{OLS in matrix notation}
	The general econometric model:
	\begin{center}
		$y_i = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki} + u_i$
	\end{center}
	Can be written in matrix notation as:
	\begin{center}
		$y = X \beta + u$
	\end{center}
	Let's call $\hat{u}$ the vector of estimated residuals ($\hat{u} \neq u$):
	\begin{center}
		$\hat{u} = y - X \hat{\beta}$
	\end{center}
	The objective of OLS is to minimize the SSR:
	\begin{center}
		Min $\SSR =$ Min $\sum_{i=1}^n \hat{u}_i^2 = $ Min $\hat{u}^T \hat{u}$
	\end{center}
	\begin{itemize}[leftmargin=*]
		\item Defining $\hat{u}^T \hat{u}$:
		\begin{center}
			$\hat{u}^T \hat{u} = (y - X \hat{\beta})^T (y - X \hat{\beta}) =$ \\
			$= y^T y -2 \hat{\beta}^T X^T y + \hat{\beta}^T X^T X \hat{\beta}$
		\end{center}
		\item Minimizing $\hat{u}^T \hat{u}$:
		\begin{center}
			$\frac{\partial \hat{u}^T \hat{u}}{\partial \hat{\beta}} = -2 X^T y +2 X^T X \hat{\beta} = 0$ \\
			$\hat{\beta} = (X^T X)^{-1} (X^T y)$ \\
			\scalebox{0.85}{
				$
				\begin{bmatrix}
					\beta_0 \\
					\beta_1 \\
					\vdots \\
					\beta_k
				\end{bmatrix}
				=
				\begin{bmatrix}
					n        & \sum x_1     & \hdots & \sum x_k     \\
					\sum x_1 & \sum x_1^2   & \hdots & \sum x_1 x_k \\
					\vdots   & \vdots       & \ddots & \vdots       \\
					\sum x_k & \sum x_k x_1 & \hdots & \sum x_k^2
				\end{bmatrix}
				^{-1}
				\cdot
				\begin{bmatrix}
					\sum y \\
					\sum y x_1 \\
					\vdots \\
					\sum y x_k
				\end{bmatrix}
				$
			}
		\end{center}
		The second derivative is $\frac{\partial^2 \hat{u}^T \hat{u}}{\partial \hat{\beta}^2} = X^T X > 0$ (is a min.)
	\end{itemize}
	\subsection*{Variance-covariance matrix}
		\begin{center}
			$\Var(\hat{\beta}) = \hat{\sigma}^2 (X^T X)^{-1} =$
			\scalebox{0.67}{
			$
			\begin{bmatrix}
				\Var(\hat{\beta}_0) & \Cov(\hat{\beta}_0, \hat{\beta}_1) & \hdots & \Cov(\hat{\beta}_0, \hat{\beta}_k) \\
				\Cov(\hat{\beta}_1, \hat{\beta}_0) & \Var(\hat{\beta}_1) & \hdots & \Cov(\hat{\beta}_1, \hat{\beta}_k) \\
				\vdots & \vdots & \ddots & \vdots \\
				\Cov(\hat{\beta}_k, \hat{\beta}_0) & \Cov(\hat{\beta}_k, \hat{\beta}_1) & \hdots & \Var(\hat{\beta}_k)
			\end{bmatrix}
			$
			}
		\end{center}
		\quad where: $\hat{\sigma}^2 = \frac{\hat{u}^T \hat{u}}{n - k - 1}$ \\
		The standard errors are in the diagonal of:
		\begin{center}
			$\se(\hat{\beta}) = \sqrt{\Var(\hat{\beta})}$
		\end{center}
	\subsection*{Error measures}
		\begin{itemize}[leftmargin=*]
			\item $\SSR = \hat{u}^T \hat{u} = y^T y - \hat{\beta}^T X^T y = \sum(y_i - \hat{y}_i)^2$
			\item $\SSE = \hat{\beta}^T X^T y - n \overline{y}^2 = \sum(\hat{y}_i - \overline{y})^2$
			\item $\SST = \SSR + \SSE = y^T y - n \overline{y}^2 = \sum(y_i - \overline{y})^2$
		\end{itemize}
	\subsection*{R-squared}
	\begin{center}
		$R^2 = \frac{\SSE}{\SST} = 1 - \frac{\SSR}{\SST}$
	\end{center}
	The R-squared corrected by degrees of freedom:
	\begin{center}
		$\overline{R}^2 = 1 - \frac{n - 1}{n - k - 1} (1 - R^2)$
	\end{center}
	
\columnbreak

\section*{Variable omission}
	Most of the time, is hard to get all relevant variables for an analysis. For example, a true model with all variables:
	\begin{center}
		$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u$
	\end{center}
	The estimated model (with the available variables):
	\begin{center}
		$\tilde{y} = \tilde{\beta}_0 + \tilde{\beta}_1 x_1 + u$
	\end{center}
	Omission of variables provoke OLS bias and inconsistency. \\
	Depending of the correlation between $x_1$ and $x_2$ and the sign of $\beta_2$, the bias on $\tilde{\beta}_1$ could be:
	\begin{center}
		\scalebox{0.9}{ % scale the table
			\begin{tabular}{| c | c | c |}
				\hline
				              & $\Corr(x_1, x_2) > 0$ & $\Corr(x_1, x_2) < 0$ \\ \hline
				$\beta_2 > 0$ & $(+) \text{ bias}$    & $(-) \text{ bias}$    \\ \hline
				$\beta_2 < 0$ & $(-) \text{ bias}$    & $(+) \text{ bias}$    \\ \hline
			\end{tabular}
		}
	\end{center}
	$(+)$ bias $\rightarrow$ $\tilde{\beta}_1$ will be higher than it should be (it includes the effect of $x_2$). $\tilde{\beta}_1 > \beta_1$ \\
	$(-)$ bias $\rightarrow$ $\tilde{\beta}_1$ will be lower than it should be (it includes the effect of $x_2$). $\tilde{\beta}_1 < \beta_1$ \\
	If $\Corr(x_1,x_2) = 0$, there is no bias on $\beta_1$, because the effect of $x_2$ will be fully picked up by the error term, $u$. \\
	There are two approaches to solve the problem:
	\begin{itemize}[leftmargin=*]
		\item Make use of proxy variables.
		\item Make use of instrumental variables (IV).
	\end{itemize}
	\subsection*{Proxy variables}
		Is the approach when a relevant variable is not available for the model because is non-observable, and there is no data.
		\begin{itemize}[leftmargin=*]
			\item A proxy variable is something related with the non-observable variable that has data available.
		\end{itemize}
		For example, the intellectual coefficient is a proxy variable for a subject's capacity (non-observable).
	\subsection*{Instrumental variables (IV)}
		When proxy variables are not available, the alternative approach is to look for a variable, let's call it $z$, that has a relation with $x$. The $z$ variable must meet the following requirements to be called an Instrumental Variable (IV):
		\begin{center}
			$\Cov(z,u) = 0$ (exogeneity) \\
			$\Cov(z,x) \neq 0$ (relevance)
		\end{center}
		This method let the omitted variable in the error term, but instead of estimate the model by OLS, it utilizes a method that recognize the presence of an omitted variable. This method can also solve error measurements.
	\subsection*{TSLS}
		This is a method to estimate a model with multiple instrumental variables. \\
		The $\Cov(z,u) = 0$ requirement can be relaxed, but there has to be a minimum of variables that satisfies it. \\
		Can have multicollinearity problems. \\
		\textbf{Some tests}:
		\begin{itemize}[leftmargin=*]
			\item Endogeneity tests:  is TSLS better than OLS when there are no endogenous variables? Do we really need TSLS? $\rightarrow$ Hausman test $\rightarrow H_0$: OLS is consistent (it is better to use OLS).
			\item Is there too many IV? $\rightarrow$ Sagan test $\rightarrow H_0$: all IV seem ok
		\end{itemize}
	
\section*{Information criterion}
	It is used to compare models with different number of parameters ($k$). The general formula:
	\begin{center}
		$\Cr(k) = \log(\frac{\SSR}{n}) + c_n \varphi (k)$
	\end{center}
	where:
	\begin{itemize}[leftmargin=*]
		\item $\SSR$ is the Sum of Squared Residuals from a model of order $k$.
		\item $c_n$ is a sequence indexed by the sample size.
		\item $\varphi(k)$ is a function that penalizes large $k$ orders.
	\end{itemize}
	The order of $k$ that minimizes the criterion is chosen. \\
	The $c_n \varphi(k)$ function:
	\begin{itemize}[leftmargin=*]
		\item Akaike: $\AIC(k) = \log(\frac{\SSR}{n}) + \frac{2}{n} k$
		\item Hannan-Quinn: $\HQ(k) = \log(\frac{\SSR}{n}) + \frac{2 \log(\log(n))}{n} k$
		\item Schwarz: $\Sc(k) = \log(\frac{\SSR}{n}) + \frac{\log(n)}{n} k$
	\end{itemize}
	$\Sc(k) \leq \HQ(k) \leq \AIC(k)$

\columnbreak


\section*{Incorrect functional forms}
	Ramsey RESET test: it test the specification errors of a regression. $H_0$: the model is correctly specified.

\section*{VAR}

The VAR model general form:

$y_t = A_1 y_{t-1} + ... + A_p y_{t-p} + B_0 x_t + ... + B_q x_{t-q} + CD_t + u_t$

where:

\begin{itemize}[leftmargin=*]
	\item $y_t = (y_{1t}, ..., y_{Kt})'$ is a vector of $K$ observable endogenous variables.
	\item $x_t = (x_{1t}, ..., x_{Mt})'$ is a vector of $M$ observable exogenous or unmodelled variables.
	\item $D_t$ contains all deterministic variables which may consist of a constant, a linear trend, seasonal dummy variables...
	\item $u_t$ is a $K$-dimensional unobservable zero mean white noise process with positive definite covariance matrix $E(u_t u_t' = \Sigma_u)$.
	\item The $A_i$, $B_j$ and $C$ are parameter matrices of suitable dimension.
\end{itemize}

For example, a model with two endogenous variables (with two lags), an exogenous contemporaneous variable, a constant ($const$) and a trend ($Trend_t$):

\scalebox{0.65}{
	\\ $y_{1t} = a_{11,1} y_{1,t-1} + a_{12,1} y_{2,t-1} + a_{11,2} y_{1,t-2} + a_{12,2} y_{2,t-2} + b_{11} x_t  + c_{11} + c_{12} Trend_t + u_{1t}$
}

\scalebox{0.65}{
	\\ $y_{2t} = a_{21,1} y_{2,t-1} + a_{22,1} y_{1,t-1} + a_{21,2} y_{2,t-2} + a_{22,2} y_{1,t-2} + b_{21} x_t + c_{21} + c_{22} Trend_t + u_{2t}$
}

For example, the equations:

\scalebox{0.8}{
	$
	\begin{bmatrix}
		y_{1t} \\
		y_{2t}
	\end{bmatrix}
	= 
	A_1
	\cdot
	\begin{bmatrix}
		y_{1,t-1} \\
		y_{2,t-1}
	\end{bmatrix}
	+
	A_2
	\cdot
	\begin{bmatrix}
		y_{1,t-2} \\
		y_{2,t-2}
	\end{bmatrix}
	+
	B_0
	\cdot
	\begin{bmatrix}
		x_t
	\end{bmatrix}
	+
	C
	\cdot
	\begin{bmatrix}
		const \\
		Trend_t \\
	\end{bmatrix}
	+
	\begin{bmatrix}
		u_{1t} \\
		u_{2t}
	\end{bmatrix}
	$
}

\scalebox{0.55}{
$
\begin{bmatrix}
	y_{1t} \\
	y_{2t}
\end{bmatrix}
= 
\begin{bmatrix}
	a_{11,1} & a_{12,1} \\
	a_{21,1} & a_{22,1}
\end{bmatrix}
\cdot
\begin{bmatrix}
	y_{1,t-1} \\
	y_{2,t-1}
\end{bmatrix}
+
\begin{bmatrix}
	a_{11,2} & a_{12,2} \\
	a_{21,2} & a_{22,2}
\end{bmatrix}
\cdot
\begin{bmatrix}
	y_{1,t-2} \\
	y_{2,t-2}
\end{bmatrix}
+
\begin{bmatrix}
	b_{11} \\
	b_{21}
\end{bmatrix}
\cdot
\begin{bmatrix}
	x_t
\end{bmatrix}
+
\begin{bmatrix}
	c_{11} & c_{12} \\
	c_{21} & c_{22}
\end{bmatrix}
\cdot
\begin{bmatrix}
	const \\
	Trend_t \\
\end{bmatrix}
+
\begin{bmatrix}
	u_{1t} \\
	u_{2t}
\end{bmatrix}
$
}

\section*{VECM}
\scalebox{0.7}{
	$\Delta y_t = 
	\Pi^*
	\begin{bmatrix}
		y_{t-1} \\
		D_{t-1}^{co}
	\end{bmatrix}
	+ \Gamma_1 \Delta y_{t-1} + ... + \Gamma_{p} \Delta y_{t-p} + B_0 x_t + ... + B_q x_{t-q} + CD_t + u_t$
}
\\ where:
\begin{itemize}[leftmargin=*]
	\item $\Pi^* = \alpha \beta^{T*}$.
	\item $D_t^{co}$ contains all deterministic terms included in the cointegration relations.
	\item $D_t$ contains all remaining deterministic variables.
\end{itemize}

For example. The matrix form:

\scalebox{0.5}{
	$
	\begin{bmatrix}
		\Delta y_{1t} \\
		\Delta y_{2t}
	\end{bmatrix}
	= 
	\Pi^*
	\begin{bmatrix}
		y_{t-1} \\
		const
	\end{bmatrix}
	+
	\Gamma_1
	\cdot
	\begin{bmatrix}
		\Delta y_{1,t-1} \\
		\Delta y_{2,t-1}
	\end{bmatrix}
	+
	B_0
	\cdot
	\begin{bmatrix}
		x_t
	\end{bmatrix}
	+
	C
	\cdot
	\begin{bmatrix}
		Trend_t
	\end{bmatrix}
	+
	\begin{bmatrix}
		u_{1t} \\
		u_{2t}
	\end{bmatrix}
	$
}

\scalebox{0.5}{
	$
	\begin{bmatrix}
		\Delta y_{1t} \\
		\Delta y_{2t}
	\end{bmatrix}
	= 
	\alpha
	\left[
	\beta^T
	\begin{bmatrix}
		y_{1,t-1} \\
		y_{2,t-1}
	\end{bmatrix}
	+
	C^*
	\begin{bmatrix}
		const
	\end{bmatrix}
	\right]
	+
	\Gamma_1
	\cdot
	\begin{bmatrix}
		\Delta y_{1,t-1} \\
		\Delta y_{2,t-1}
	\end{bmatrix}
	+
	B_0
	\cdot
	\begin{bmatrix}
		x_t
	\end{bmatrix}
	+
	C
	\cdot
	\begin{bmatrix}
		Trend_t
	\end{bmatrix}
	+
	\begin{bmatrix}
		u_{1t} \\
		u_{2t}
	\end{bmatrix}
	$
}

\scalebox{0.5}{
	$
	\begin{bmatrix}
		\Delta y_{1t} \\
		\Delta y_{2t}
	\end{bmatrix}
	= 
	\begin{bmatrix}
		\alpha_{11} \\
		\alpha_{12}
	\end{bmatrix}
	\left[
	\begin{bmatrix}
		\beta_{11} & \beta_{21}
	\end{bmatrix}
	\begin{bmatrix}
		y_{1,t-1} \\
		y_{2,t-1}
	\end{bmatrix}
	+
	\begin{bmatrix}
		c^*
	\end{bmatrix}
	\begin{bmatrix}
		const
	\end{bmatrix}
	\right]
	+
	\begin{bmatrix}
		\gamma_{11} & \gamma_{12} \\
		\gamma_{21} & \gamma_{22}
	\end{bmatrix}
	\cdot
	\begin{bmatrix}
		\Delta y_{1,t-1} \\
		\Delta y_{2,t-1}
	\end{bmatrix}
	+
	\begin{bmatrix}
		b_{11} \\
		b_{21}
	\end{bmatrix}
	\cdot
	\begin{bmatrix}
		x_t
	\end{bmatrix}
	+
	\begin{bmatrix}
		c_{11} \\
		c_{21}
	\end{bmatrix}
	\cdot
	\begin{bmatrix}
		Trend_t
	\end{bmatrix}
	+
	\begin{bmatrix}
		u_{1t} \\
		u_{2t}
	\end{bmatrix}
	$
}

The VECM model general form:



\scalebox{0.8}{
	$\Delta y_t = \Pi y_{t-1} + \Gamma_1 \Delta y_{t-1} + ... + \Gamma_{p} \Delta y_{t-p} + B_0 x_t + ... + B_q x_{t-q} + CD_t + u_t$
}

where:
\begin{itemize}[leftmargin=*]
	\item $y_t = (y_{1t}, ..., y_{Kt})'$ is a vector of $K$ observable endogenous variables.
	\item $x_t = (x_{1t}, ..., x_{Mt})'$ is a vector of $M$ observable exogenous or unmodelled variables.
	\item $\Pi = \alpha \beta'$. Suppose $\mathrm{rk}(\Pi) = r = \mathrm{rk}(\alpha) = \mathrm{rk}(\beta)$
	\item $D_t$ contains all deterministic variables which may consist of a constant, a linear trend, seasonal dummy variables...
	\item $u_t$ is a $K$-dimensional unobservable zero mean white noise process with positive definite covariance matrix $E(u_t u_t' = \Sigma_u)$.
	\item The $A_i$, $B_j$ and $C$ are parameter matrices of suitable dimension.
\end{itemize}


\end{multicols}

\end{document}