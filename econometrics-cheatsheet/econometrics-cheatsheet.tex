\documentclass[10pt,landscape]{article}

% -----packages-----
\usepackage{multicol} % for multiple columns
\usepackage[landscape]{geometry} % for landscape
\usepackage{parskip} % remove text indentation
\usepackage{graphicx} % for scale tables
\usepackage[compact]{titlesec} % titles spacing
\usepackage{enumitem} % indent of lists

% -----page customization-----
\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} % margins configuration
\pagenumbering{gobble} % remove page numeration
\setlength{\parskip}{0cm} % paragraph length
% title spacing:
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}

% -----document-----
\begin{document}
\begin{multicols}{3} % set page columns to 3

\textbf{\Large Econometrics Cheat Sheet 1.0}

\begin{scriptsize}
By Marcelo Moreno - University King Juan Carlos
\end{scriptsize}

\section*{Basic concepts}
\subsection*{Definitions}

\textbf{Econometrics} - is a social science discipline with the objective of quantify the relationships between economic agents, contrast economic theories and evaluate and implement government and business policies.

\textbf{Econometric model} - is a simplified representation of the reality to explain economic phenomena.

\subsection*{Data types}

\textbf{Cross section} - data taken at a given moment in time, an static "photo". Order does not matter.

\textbf{Temporal series} - observation of one/many variable/s across time. Order does matter.

\textbf{Panel data} - consist of a temporal series for each observation of a cross section.

\textbf{Pooled cross sections} - combines cross sections from different temporal periods.

\subsection*{Phases of an econometric model}

\begin{enumerate}[leftmargin=*]
\setlength{\multicolsep}{0pt}
\begin{multicols}{2}
\item Specification
\item Estimation
\columnbreak
\item Validation
\item Utilization
\end{multicols}
\end{enumerate}

\subsection*{Econometric model assumptions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CUT TO 4 OR 5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Under this assumptions the estimators of the parameters will present "good properties". \textbf{Extended Gauss Markov assumptions}:

\begin{enumerate}[leftmargin=*]
\item Parameters linearity. $y$ must be a linear function of the $\beta$'s.
\item The sample (data) taken from the population is random. Characteristics:
\begin{itemize}[leftmargin=*]
\item Independence: all the co-variances between the $x$'s are zero.
\item Identical distribution: all the expected values and variances of the observations are the same.
\end{itemize}
\item $E(u | x_1, ..., x_k) = 0$, the estimations are unbiased. Implications:
\begin{itemize}[leftmargin=*]
\item $E(u) = 0$, there are no systematic errors.
\item $Cov(u, x_1) = ... = Cov(u, x_k) = 0$, there are no relevant variables left out the model.
\item $E(y | x_1, ..., x_k) = \beta_0 + \beta_1 x_1 + ... + \beta_k x_k$, the linear relation between $y$ and the $x$'s is fulfilled, at least in average.
\end{itemize}
\item Homoscedasticity: $Var(u_i | x_{1i},  ..., x_{ki}) = \sigma^2$, the variability of the residual is the same for all levels of $x$.
\item No auto-correlation: $Cov(u_i, u_j | x) = 0$, the residuals do not contain information about other residuals.
\item The residuals are normally distributed, $u \sim N(0, \sigma)$
\item No multicollinearity: none of the independent variables is constant nor exist an exact (or approximate) linear relation between them, they are linearly independents.
\item The number of available data is greater than $k+1$ (parameters to estimate).
\end{enumerate}

\textbf{Efficiency of OLS estimators, Gauss-Markov Theorem}. In the context of the simple or multiple linear regression model, the OLS estimators of the parameters are those with the lowest variance between the lineal and unbiased estimators.

\subsection*{Central Limit Theorem}
% MORE EXPLANATION
Under the CLT, $\hat{\beta}_j$ is a consistent estimator of the population parameter $\beta_i$.

$p lim \hat{\beta}_i = \beta_i$

The Central Limit Theorem allow us to obtain (asymptotically):

$\frac{\hat{\beta}_i - \beta_i}{s(\hat{\beta}_i)} \sim N(0,1)$

\section*{Ordinary Least Squares}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ADD MATRICIAL FORM DETAILS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{OLS objective} - minimize the square sum of residuals:

$Min \sum_{i=1}^n \hat{u}_i^2$

\subsection*{Simple regression model}

Equation: $y_i = \beta_0 + \beta_1 x_{1i} + u_i$

Estimation: $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$

Residuals: $\hat{u}_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)$

With:

$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$

$\hat{\beta}_1 = \frac{Cov(y, x)}{Var(x)}$

\subsection*{Multiple regression model}

Equation: $y_i = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki} + u_i$

Estimation: $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_{1i} + ... + \hat{\beta}_k x_{ki}$

Residuals: $\hat{u}_i = y_i - \hat{y}_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i + ... + \hat{\beta}_k x_{ki})$

With:

$\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}_1 - ... - \hat{\beta}_k \overline{x}_k$

$\hat{\beta}_j = \frac{Cov(y, resid(x_j))}{Var(resid(x_j))}$

In matrix form:

$\hat{\beta} = (X^T X)^{-1} X^T Y$

\subsection*{Interpretation of coefficients}

\scalebox{0.8}{
\begin{tabular}{ | l | c | c | c | }
	\hline
	Model & Dependent & Independent & $\beta_1$ interpretation \\
	\hline
	Level-level & $y$ & $x$ & $\Delta y = \beta_1 \Delta x$ \\ 
	\hline
	Level-log & $y$ & $log(x)$ & $\Delta y = (\beta_1/100)[1 \% \Delta x]$ \\  
	\hline
	Log-level & $log(y)$ & $x$ & $\% \Delta y = (100 \beta_1) \Delta x$ \\
	\hline
	Log-log & $log(y)$ & $log(x)$ & $\% \Delta y = \beta_1 \% \Delta x$ \\
	\hline
	Quadratic & $y$ & $x + x^2$ & $\Delta y = (\beta_1 + 2 \beta_2 x) \Delta x$ \\
	\hline
\end{tabular}
}

\subsection*{Goodness of the fit, R-squared}

Is a measure of the goodness of the fit (how the OLS fit to the data):

$R^2 = \frac{\sum_i (\hat{y}_i - \overline{y}_i)^2}{\sum_i (y_i - \overline{y}_i)^2} = 1 - \frac{\sum_i \hat{u}_i^2}{nS_y^2}$

\begin{itemize}[leftmargin=*]
\item Takes values between 0 (no lineal explanation of the variations of $y$) and 1 (total explanation of the variations of $y$).
\item Measures the percentage of variation of $y$ that is linearly explained by the variations of $x$'s.
\item When the number of regressors increment, the value of the r-squared increments as well, whatever the new variables are relevant or not.
\end{itemize}

For eliminate the last point, there is an r-squared corrected by degrees of freedom:

$\overline{R}^2 = 1 - \frac{n-1}{n-k-1} \frac{\sum_{i=1}^n \hat{u}_i^2}{\sum_{i=1}^n (y_i - \overline{y}_i)^2} = 1 - \frac{n-1}{n-k-1} (1-R^2)$

For big sample sizes:

$\overline{R}^2 \approx R^2$

\subsection*{Error measures}

Total Sum of Squares: $SST = \sum_{i=1}^n (y_i - \overline{y}_i)^2$

Explained Sum of Squares: $SSE = \sum_{i=1}^n (\hat{y}_i - \overline{y}_i)^2$

Residual Sum of Squares: $SSR = \sum_{i=1}^n u_i^2$

Standard error of the regression: $\hat{\sigma} = \sqrt{\frac{\sum_{i=1}^n \hat{u}_i^2}{n-k-1}}$

\section*{Hypothesis testing}

\subsection*{The basics of hypothesis testing}

An hypothesis test is a rule designed to explain from a sample, if exist evidence or not to reject an hypothesis that is made of one or more population parameters.

Elements of an hypothesis contrast:

\begin{itemize}[leftmargin=*]
\item Null hypothesis ($H_0$): is the hypothesis that you want to contrast.
\item Alternative hypothesis ($H_1$): is the hypothesis that cannot be rejected when the null hypothesis is rejected.
\item Statistic of contrast: is a random variable with a known distribution that allow us to see if we reject (or not) the null hypothesis.
\item Significance level ($\alpha$): is the probability of rejecting the null hypothesis being true (type I error). Is chosen by who conduct the contrast. Commonly is 0.10, 0.05, 0.01 or 0.001
\item Critic value: is the value that, for a determined value of $\alpha$, determines the reject (or not) of the null hypothesis.
\item p-value: is the highest level of significance for what we do not reject (accept) the null hypothesis ($H_0$).
\end{itemize}

\textbf{The rule is}: if p-value is lower than $\alpha$, there is evidence at that given $\alpha$ to reject the null hypothesis (accept the alternative instead).

\subsection*{Individual contrasts}

Under the premise of normality of the residuals, contrast if a given parameter is significantly different from a given value.

\begin{itemize}[leftmargin=*]
\item $H_0: \beta_j = \theta$
\item $H_1: \beta_j \neq \theta$
\end{itemize}

Under $H_0$:

$t = \frac{\hat{\beta}_j - \beta_j}{s(\hat{\beta}_j)} \sim t_{n-k-1, \alpha/2}$

If $\mid t \mid > t_{n-k, \alpha/2}$ there is evidence to reject the null hypothesis.

\textbf{Individual significance contrasts} - contrast if a given parameter is significantly different from zero.

\begin{itemize}[leftmargin=*]
\item $H_0: \beta_j = 0$
\item $H_1: \beta_j \neq 0$
\end{itemize}

Under $H_0$:

$t = \frac{\hat{\beta}_j - 0}{s(\hat{\beta}_j)} \sim t_{n-k-1, \alpha/2}$

If $\mid t \mid > t_{n-k, \alpha/2}$ there is evidence to reject the null hypothesis.

\section*{Confidence intervals}

Under the normality of the residuals requirement, the confidence intervals at $1 - \alpha$ confidence can be calculated:

$\hat{\beta}_j \mp t_{n-k-1, \alpha/2} s(\hat{\beta}_j)$

\section*{The F contrast}

It uses a non restricted model and a restricted model to do assumptions about the parameters.

\begin{itemize}[leftmargin=*]
\item Non restricted model: is the model on which we want to make the hypothesis contrast.
\item Restricted model: is the model on which the hypothesis that we want to contrast have been imposed.
\end{itemize}

Then, looking at the errors, there is:

\begin{itemize}[leftmargin=*]
\item $\sum_{i=1}^n \hat{u}_N^2$: is the sum of the OLS residuals of the non restricted model (SRN).
\item $\sum_{i=1}^n \hat{u}_R^2$: is the sum of the OLS residuals of the restricted model (SRR).
\end{itemize}

Then: $F = \frac{SRR - SRN}{SRN} \frac{(n-K-1)}{q} \sim F_{q, n-K-1}$

Where $K$ is the number of parameters of the non restricted model and $q$ is the number of linear hypothesis.

When $F_{q, n-K-1} < F$, there is evidence to reject the null hypothesis.

\section*{Dummy variables and structural change}

Dummy (or binary) variables are used for qualitative information: sex, civil state, etc.

\begin{itemize}[leftmargin=*]
\item The dummy variables get the value of 1 in a given category, and 0 on the rest.
\item Dummy variables are used to analyze and modeling structural changes in the model parameters.
\end{itemize}

\subsection*{Structural change}

We denominate structural changes to the modifications in the value of the parameters of the models for different sub-populations.

The position of the dummy variable matters:
\begin{itemize}[leftmargin=*]
\item On the constant, their associate parameter represents the difference in mean between the values.
\item On the parameters that determines the slope of the regression line, the associate parameter represents the difference in the effect between the values.
\end{itemize}

\textbf{The Chow's structural contrast}

When we want to analyze the existence of structural changes in all the model parameters, is more common to use a particular expression of the F contrast known as the Chow's contrast.

It defines two non restricted models (with structural change):

$y_i = \beta_0^A + \beta_1^A x_{1i} + ... + \beta_k^A x_{ki} + u_i$ from sub-sample A

$y_i = \beta_0^B + \beta_1^B x_{1i} + ... + \beta_k^B x_{ki} + u_i$ from sub-sample B

Restricted model (without structural change):

$y_i = \beta_0 + \beta_1 x_{1i} + ... + \beta_k x_{ki} + u_i$

With the restriction:

$H_0: \beta_j^A = \beta_j^B; j=0,1,...,k$

\begin{itemize}[leftmargin=*]
\item Be SRN the sum of the OLS square residuals of the non restricted model: $SRN = SR_A + SR_B$
\item Be SRR the sum of the OLS square residuals of the restricted model.
\end{itemize}

Then:

$F = \frac{SRR - SRN}{SRN} \frac{n-2(k+1)}{k+1} \sim F_{k+1,n-2(k+1)}$

If $F_{q, n-K-1} < F$, there is evidence to reject the null hypothesis.

\section*{Multicollinearity}

If there is exact multicolineality, the equation system of OLS cannot be solved due to infinite solutions.

\begin{itemize}[leftmargin=*]
\item Approximate multicollinearity: when one or more variables are almost a constant or there is a linear relation between them. In this context, there is not a problem, given the classic requirements of OLS, and the inference is valid. But, there are some empiric consequences of this:
\begin{itemize}[leftmargin=*]
\item Small sample variations can induce to big variations in the OLS estimations.
\item The variance of the OLS estimator of the $x$'s that are collinear $Var(\hat{\beta}_j)$ increments, then, the inference of the parameter is affected $\rightarrow$ The estimation of the parameter is very imprecise (big confidence interval).
\end{itemize}
\end{itemize}

Calculating the Variance Inflation Factor to analyze multicollinearity problems:

$VIF(\hat{\beta}_j) = \frac{1}{1-R_j^2}$

Indicates the increment of $Var(\hat{\beta}_j)$ because of the multicollinearity.

\begin{itemize}[leftmargin=*]
\item If it is bigger than 10, indicates that there are multicollinearity problems.
\item From 4 onwards, it is advisable to analyze in more detail if there might be
multicollinearity.
\end{itemize}

One typical characteristic of multicollinearity is that the regression coefficients of the model are not individually different from zero (because the high variances), but jointly they are different from zero.

\columnbreak

\section*{Heteroscedasticity}

The residuals $u_i$ of the population regression function do not have the same variance $\sigma^2$:

$Var(u|x) = Var(y|x) \neq \sigma^2$

\subsection*{Consequences}

\begin{itemize}[leftmargin=*]
\item The estimators still are unbiased.
\item The estimators still are consistent.
\item The variance of the estimators is biased: the construction of confidence intervals and the hypothesis contrast are not reliable.
\end{itemize}

In this context, OLS is not an unbiased lineal estimator of minimum variance. There is an alternate unbiased lineal estimator of minimum variance denominated estimator of least weighted squares (OLWS) or least generalized squares (LGS).

\subsection*{Detection}

Plots. Look for structures in plots with the square residuals) and contrasts: Park test, Goldfield-Quandt, Bartlett, Breush-Pagan, CUSUMQ, Spearman, White.

White test null hypothesis:

$H_0 = Homoscedasticity$

\subsection*{Correction}

\begin{itemize}[leftmargin=*]
	\item When the variance structure is known, use weighted least squares.
	\item When the variance structure is not known: make assumptions of the possible structure and apply weighted least squares (factible weighted least squares)
	\item Supposing that $\sigma_i^2$ is proportional to $x_i^2$, divide by $x_i$
\end{itemize}

\subsection*{Auto-correlation}

The "natural" context of this phenomena is in temporal series.

The residual of any observation, $u_i$ is correlated with the residual of any other observation. The observations are not independent $E(u_i,u_j) \neq 0; i \neq j$


\section*{Regression Analysis}
Study and predict the mean value of a variable regarding the base of fixed values of other variables.
We usually use Ordinary Least Squares (OLS).

\section*{Correlation Analysis}
The correlation analysis not distinguish between dependent and independent variables.
\textbf{Simple Correlation}
Measure the grade of lineal association between two variables.

\end{multicols}
\end{document}
